import { useState, useRef, useCallback } from 'react';
import Groq from 'groq-sdk';
import { ConnectionStatus } from '../types';

interface UseLiveSessionProps {
  apiKey: string | undefined;
  systemInstruction: string;
}

export const useLiveSession = ({ apiKey, systemInstruction }: UseLiveSessionProps) => {
  const [status, setStatus] = useState<ConnectionStatus>('disconnected');
  const [hostInterjection, setHostInterjection] = useState<string>('');
  const [currentTranscript, setCurrentTranscript] = useState<string>('');
  
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const groqClientRef = useRef<Groq | null>(null);
  const currentTranscriptRef = useRef<string>('');
  const isLivePausedRef = useRef(false);
  const conversationHistoryRef = useRef<Array<{ role: string; content: string }>>([]);
  const recordingTimeoutRef = useRef<ReturnType<typeof setTimeout> | null>(null);
  const silenceTimeoutRef = useRef<ReturnType<typeof setTimeout> | null>(null);
  const lastTranscriptLengthRef = useRef(0);
  const consecutiveFailuresRef = useRef(0);
  
  // FIX: Add missing refs for resume functionality
  const audioOnlyStreamRef = useRef<MediaStream | null>(null);
  const selectedMimeTypeRef = useRef<string>('');

  const connect = useCallback(async (stream: MediaStream) => {
    if (!apiKey) {
      console.error("No API Key provided");
      setStatus('error');
      return;
    }

    setStatus('connecting');
    isLivePausedRef.current = false;
    consecutiveFailuresRef.current = 0;

    groqClientRef.current = new Groq({
      apiKey: apiKey,
      dangerouslyAllowBrowser: true
    });

    try {
      const audioTracks = stream.getAudioTracks();
      if (audioTracks.length === 0) {
        throw new Error('No audio track available');
      }

      const audioOnlyStream = new MediaStream(audioTracks);
      audioOnlyStreamRef.current = audioOnlyStream; // FIX: Store for resume
      console.log('✓ Audio-only stream created');

      // Prioritize WAV format
      let selectedMimeType = '';
      const formats = [
        'audio/wav',
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/ogg'
      ];

      for (const format of formats) {
        if (MediaRecorder.isTypeSupported(format)) {
          selectedMimeType = format;
          break;
        }
      }

      selectedMimeTypeRef.current = selectedMimeType; // FIX: Store for resume
      console.log('✓ Selected format:', selectedMimeType || 'default');

      startRecording(audioOnlyStream, selectedMimeType);
      setStatus('connected');

    } catch (error) {
      console.error("✗ Failed to start:", error);
      setStatus('error');
    }
  }, [apiKey, systemInstruction]);

  const startRecording = useCallback((audioOnlyStream: MediaStream, selectedMimeType: string) => {
    if (isLivePausedRef.current || !groqClientRef.current) return;

    audioChunksRef.current = [];

    const options: MediaRecorderOptions = selectedMimeType
      ? { mimeType: selectedMimeType }
      : {};

    if (selectedMimeType === 'audio/wav') {
      options.audioBitsPerSecond = 128000;
    }

    mediaRecorderRef.current = new MediaRecorder(audioOnlyStream, options);

    mediaRecorderRef.current.ondataavailable = (event) => {
      if (event.data.size > 0) {
        audioChunksRef.current.push(event.data);
      }
    };

    mediaRecorderRef.current.onstop = async () => {
      if (audioChunksRef.current.length > 0 && !isLivePausedRef.current) {
        const mimeType = mediaRecorderRef.current?.mimeType || 'audio/webm';
        const audioBlob = new Blob(audioChunksRef.current, { type: mimeType });

        if (audioBlob.size < 5000) {
          console.log(`⊘ Small chunk: ${(audioBlob.size / 1024).toFixed(1)}KB`);
        } else {
          try {
            console.log(`→ ${(audioBlob.size / 1024).toFixed(1)}KB`);
            
            const fileName = mimeType.includes('wav') ? 'audio.wav' : 
                            mimeType.includes('webm') ? 'audio.webm' : 'audio.ogg';
            
            const audioFile = new File([audioBlob], fileName, { type: mimeType });

            const transcription = await groqClientRef.current!.audio.transcriptions.create({
              file: audioFile,
              model: 'whisper-large-v3',
              response_format: 'json',
              language: 'en',
              temperature: 0.0,
            });

            consecutiveFailuresRef.current = 0;

            if (transcription.text?.trim()) {
              const newText = transcription.text.trim();
              console.log('✓', newText);
              
              if (newText.length > 2) {
                currentTranscriptRef.current += (currentTranscriptRef.current ? ' ' : '') + newText;
                setCurrentTranscript(currentTranscriptRef.current);
                
                if (silenceTimeoutRef.current) {
                  clearTimeout(silenceTimeoutRef.current);
                }

                silenceTimeoutRef.current = setTimeout(async () => {
                  if (currentTranscriptRef.current.length > lastTranscriptLengthRef.current + 15) {
                    await generateHostResponse();
                    lastTranscriptLengthRef.current = currentTranscriptRef.current.length;
                  }
                }, 7000);
              }
            }
          } catch (error: any) {
            consecutiveFailuresRef.current++;
            
            if (consecutiveFailuresRef.current % 3 === 0) {
              console.error(`✗ Failed (${consecutiveFailuresRef.current}x)`);
              if (error.response) {
                console.error('API:', error.response.data);
              } else {
                console.error('Error:', error.message);
              }
            }
            
            if (consecutiveFailuresRef.current > 15) {
              console.error('✗ Too many failures');
              consecutiveFailuresRef.current = 0;
            }
          }
        }
      }

      if (recordingTimeoutRef.current) {
        clearTimeout(recordingTimeoutRef.current);
      }

      // Restart recording if not paused
      if (!isLivePausedRef.current) {
        startRecording(audioOnlyStream, selectedMimeType);
      }
    };

    mediaRecorderRef.current.start();
    console.log('✓ Recording started');

    // Stop after 10 seconds
    recordingTimeoutRef.current = setTimeout(() => {
      if (mediaRecorderRef.current?.state === 'recording') {
        mediaRecorderRef.current.stop();
      }
    }, 10000);
  }, []);

  const generateHostResponse = async () => {
    if (!groqClientRef.current || isLivePausedRef.current) return;
    
    try {
      const messages = [
        { role: 'system', content: systemInstruction },
        ...conversationHistoryRef.current,
        { role: 'user', content: currentTranscriptRef.current }
      ];

      console.log('→ AI...');
      const completion = await groqClientRef.current.chat.completions.create({
        model: 'llama-3.3-70b-versatile',
        messages: messages as any,
        temperature: 0.7,
        max_tokens: 150,
      });

      const response = completion.choices[0]?.message?.content;
      
      if (response) {
        console.log('✓ AI:', response);
        setHostInterjection(response);
        
        conversationHistoryRef.current.push(
          { role: 'user', content: currentTranscriptRef.current },
          { role: 'assistant', content: response }
        );

        if (conversationHistoryRef.current.length > 20) {
          conversationHistoryRef.current = conversationHistoryRef.current.slice(-20);
        }

        setTimeout(() => setHostInterjection(''), 8000);
      }
    } catch (error) {
      console.error('✗ AI error:', error);
    }
  };

  const disconnect = useCallback(() => {
    if (mediaRecorderRef.current?.state !== 'inactive') {
      mediaRecorderRef.current?.stop();
    }
    if (recordingTimeoutRef.current) {
      clearTimeout(recordingTimeoutRef.current);
    }
    if (silenceTimeoutRef.current) {
      clearTimeout(silenceTimeoutRef.current);
    }

    audioChunksRef.current = [];
    conversationHistoryRef.current = [];
    consecutiveFailuresRef.current = 0;
    audioOnlyStreamRef.current = null;
    selectedMimeTypeRef.current = '';
    setStatus('disconnected');
    setHostInterjection('');
    console.log('✓ Session closed');
  }, []);

  const pause = useCallback(() => {
    isLivePausedRef.current = true;
    if (mediaRecorderRef.current?.state === 'recording') {
      mediaRecorderRef.current.stop();
    }
    if (recordingTimeoutRef.current) {
      clearTimeout(recordingTimeoutRef.current);
    }
    console.log('⏸ Paused');
  }, []);

  const resume = useCallback(() => {
    isLivePausedRef.current = false;
    consecutiveFailuresRef.current = 0;
    
    // FIX: Now these refs exist!
    if (audioOnlyStreamRef.current && selectedMimeTypeRef.current) {
      startRecording(audioOnlyStreamRef.current, selectedMimeTypeRef.current);
      console.log('▶ Resumed');
    }
  }, [startRecording]);

  const resetTranscript = useCallback(() => {
    currentTranscriptRef.current = '';
    setCurrentTranscript('');
    conversationHistoryRef.current = [];
    lastTranscriptLengthRef.current = 0;
    console.log('↻ Reset');
  }, []);

  return {
    status,
    connect,
    disconnect,
    pause,
    resume,
    hostInterjection,
    currentTranscript,
    resetTranscript
  };
};
